{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World and Grid Node\n",
    "\n",
    "Project done for fun, this implementation of gridworld was to better understand the algorithms and experiment with various permutations of hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class GridNode\n",
    "Primary implementation for Gridnode\n",
    "params:\n",
    "    @reward: specifies reward for transitioning away from a given gridnode \n",
    "Returns:\n",
    "    GridNode\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GridNode():\n",
    "\n",
    "    def __init__(self, reward) -> None:\n",
    "        self.reward = reward\n",
    "        self.v = 0\n",
    "        self.actions = {\n",
    "            'up': {\n",
    "                'movement': (0, -1),\n",
    "                'p': .25,\n",
    "                'r': 0\n",
    "            },\n",
    "            'left': {\n",
    "                'movement': (-1, 0),\n",
    "                'p': .25,\n",
    "                'r': 0\n",
    "            },\n",
    "            'right': {\n",
    "                'movement': (1, 0),\n",
    "                'p': .25,\n",
    "                'r': 0\n",
    "            },\n",
    "            'down': {\n",
    "                'movement': (0, 1),\n",
    "                'p': .25,\n",
    "                'r': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "    Changes value of action dict\n",
    "    \"\"\"\n",
    "\n",
    "    def change_action(self, action, key, change):\n",
    "        self.actions[action][key] = change\n",
    "\n",
    "    \"\"\"\n",
    "    Returns movement to in order to transition to a given node given an action as well as reward for leaving node\n",
    "    \"\"\"\n",
    "\n",
    "    def transition(self, action):\n",
    "        coords = self.actions[action]['movement']\n",
    "        r = self.actions[action]['r']\n",
    "        p = self.actions[action]['p']\n",
    "        return coords, r, p\n",
    "\n",
    "\n",
    "class GridWorld():\n",
    "\n",
    "    def __init__(self, rows, columns, startX, startY, fall) -> None:\n",
    "        \"\"\"initializer\n",
    "\n",
    "        Args:\n",
    "            rows (int): num rows in grid\n",
    "            columns (int): num cols in grid\n",
    "            startX (int): starting posx\n",
    "            startY (int): starting posy\n",
    "            fall (int): reward for falling\n",
    "        \"\"\"\n",
    "        nodes = [GridNode(reward=0) for x in range(rows * columns)]\n",
    "        self.lenNodes = rows * columns\n",
    "        self.grid = np.array(nodes, dtype=GridNode).reshape(rows, columns)\n",
    "        self.cur = self.grid[startX, startY]\n",
    "        for i in range(rows):\n",
    "            #first row\n",
    "            self.grid[0, i].change_action(action='up',\n",
    "                                          key='movement',\n",
    "                                          change=(0, 0))\n",
    "            self.grid[0, i].change_action(action='up', key='r', change=fall)\n",
    "            #first columns\n",
    "            self.grid[i, 0].change_action(action='left',\n",
    "                                          key='movement',\n",
    "                                          change=(0, 0))\n",
    "            self.grid[i, 0].change_action(action='left', key='r', change=fall)\n",
    "            #last columns\n",
    "            self.grid[i, -1].change_action(action='right',\n",
    "                                           key='movement',\n",
    "                                           change=(0, 0))\n",
    "            self.grid[i, -1].change_action(action='right',\n",
    "                                           key='r',\n",
    "                                           change=fall)\n",
    "            #last row\n",
    "            self.grid[-1, i].change_action(action='down',\n",
    "                                           key='movement',\n",
    "                                           change=(0, 0))\n",
    "            self.grid[-1, i].change_action(action='down', key='r', change=fall)\n",
    "\n",
    "    def set_special(self, row, column, reward, actions, targetcol, targetrow):\n",
    "        \"\"\"sets special node in the grid\n",
    "\n",
    "        Args:\n",
    "            row (int): grid row \n",
    "            column (int): grid col\n",
    "            reward (int): new reward \n",
    "            actions (str): what action trigers reward \n",
    "            targetcol (int): new transition col \n",
    "            targetrow (int ): new transition row\n",
    "        \"\"\"\n",
    "        transCoords = (targetcol - column, targetrow - row)\n",
    "\n",
    "        for action in actions:\n",
    "            self.grid[row, column].change_action(action=action,\n",
    "                                                 key='r',\n",
    "                                                 change=reward)\n",
    "            self.grid[row, column].change_action(action=action,\n",
    "                                                 key='movement',\n",
    "                                                 change=transCoords)\n",
    "\n",
    "    def peek(self, state, action):\n",
    "        \"\"\" peeks transition, returns rewards, probability and transition node given a the transition \n",
    "        Args:\n",
    "            state (GridNode): cur grid node \n",
    "            action (str): action to take\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        row, col = np.where(self.grid == state)\n",
    "        (nx, ny), r, p = state.transition(action)\n",
    "        newCords = (row + ny, col + nx)\n",
    "\n",
    "        return r, newCords, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "\n",
    "    def __init__(self, actions) -> None:\n",
    "        self.actions = actions\n",
    "\n",
    "    def pick(self, a):\n",
    "        pass\n",
    "\n",
    "    def pa(self, a):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \"\"\"\n",
    "    Random policy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions) -> None:\n",
    "        super().__init__(actions)\n",
    "\n",
    "    def pick(self):\n",
    "        \"\"\"\n",
    "        randomly pick from any action\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.actions)\n",
    "\n",
    "    def pa(self):\n",
    "        \"\"\"\n",
    "        returns prob of picking any action\n",
    "\n",
    "        \"\"\"\n",
    "        return 1 / len(self.actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_argmax(ar):\n",
    "    \"\"\"randomly breaks ties of equal valued indexes \n",
    "\n",
    "    Args:\n",
    "        ar (np.array): array to argmax\n",
    "    \"\"\"\n",
    "    m = np.where(ar == max(ar))[0]\n",
    "    return np.random.choice(m)\n",
    "\n",
    "\n",
    "class DP():\n",
    "\n",
    "    def __init__(self, grid: GridWorld, policy: Policy, gamma) -> None:\n",
    "        self.grid = grid\n",
    "        self.v = np.zeros(grid.lenNodes)\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.actions = policy.actions\n",
    "\n",
    "    def solve():\n",
    "        pass\n",
    "\n",
    "\n",
    "class Solver(DP):\n",
    "\n",
    "    def __init__(self, grid: GridWorld, policy: Policy, gamma) -> None:\n",
    "        super().__init__(grid, policy, gamma)\n",
    "\n",
    "    def policyEvaluation(self, epsilon):\n",
    "        \"\"\"follows the policy evaluation algorithm found in page 90 of Sutto & Barto\n",
    " \n",
    "        Args:\n",
    "            epsilon (float): error threshold epsilon\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        vs = np.zeros_like(self.grid.grid)\n",
    "        k = 0\n",
    "        while 1:\n",
    "            delta = 0\n",
    "            for col in range(np.size(self.grid.grid, 1)):\n",
    "                for row in range(np.size(self.grid.grid, 0)):\n",
    "                    v = vs[row, col]\n",
    "                    vs_t = 0\n",
    "                    for action in self.policy.actions:\n",
    "                        state = self.grid.grid[row, col]\n",
    "                        r, coords, pa = self.grid.peek(state, action)\n",
    "                        vs_t += pa * (r + self.gamma * vs[coords][0])\n",
    "                    vs[row, col] = vs_t\n",
    "                    dif = v - vs[row, col]\n",
    "                    delta = max(delta, abs(dif))\n",
    "            k += 1\n",
    "            if epsilon > delta:\n",
    "                break\n",
    "\n",
    "        return vs\n",
    "\n",
    "    def policyImprovement(self, vs, epsilon):\n",
    "        \"\"\"follows the policy improvement algorithm found in page 92 of Sutto & Barto\n",
    " \n",
    "        Args:\n",
    "            epsilon (float): error threshold epsilon\n",
    "            vs (np.array): value function for a given grid\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        same = False\n",
    "        old_p = np.array([{\n",
    "            \"v\": np.NINF,\n",
    "            \"action\": None\n",
    "        } for x in range(len(vs.flatten()))]).reshape(vs.shape)\n",
    "        m = 0\n",
    "        while not same:\n",
    "            best = np.array([{\n",
    "                \"v\": np.NINF,\n",
    "                \"action\": None\n",
    "            } for x in range(len(vs.flatten()))]).reshape(vs.shape)\n",
    "\n",
    "            for col in range(np.size(vs, 1)):\n",
    "                for row in range(np.size(vs, 0)):\n",
    "                    for action in self.policy.actions:\n",
    "                        state: GridNode = self.grid.grid[row, col]\n",
    "                        r, coords, pa = self.grid.peek(state, action)\n",
    "                        vstar = (r + self.gamma * vs[coords][0])\n",
    "                        if best[row, col]['v'] < vstar:\n",
    "                            best[row, col]['v'] = vstar\n",
    "                            best[row, col]['action'] = action\n",
    "                            state.change_action(action, 'p', 1)\n",
    "\n",
    "                    [\n",
    "                        state.change_action(a, 'p', 0)\n",
    "                        for a in self.policy.actions\n",
    "                        if a != best[row, col]['action']\n",
    "                    ]\n",
    "\n",
    "            if (old_p == best).all() or m == 20:\n",
    "                break\n",
    "            else:\n",
    "                old_p = best.copy()\n",
    "                vs = self.policyEvaluation(epsilon)\n",
    "        return best, vs\n",
    "\n",
    "    def ValueIteration(self, epsilon):\n",
    "        \"\"\"follows the policy evaluation algorithm found in page 97 of Sutto & Barto\n",
    " \n",
    "        Args:\n",
    "            epsilon (float): error threshold epsilon\n",
    "\n",
    "        \"\"\"\n",
    "        vs = np.zeros_like(self.grid.grid)\n",
    "        delta = np.infty\n",
    "        best = np.array([{\n",
    "            \"v\": np.NINF,\n",
    "            \"action\": None\n",
    "        } for x in range(len(vs.flatten()))]).reshape(vs.shape)\n",
    "        while delta > epsilon:\n",
    "            delta = 0\n",
    "            for col in range(np.size(vs, 1)):\n",
    "                for row in range(np.size(vs, 0)):\n",
    "                    state: GridNode = self.grid.grid[row, col]\n",
    "                    action_v = np.zeros_like(self.actions, dtype=np.float64)\n",
    "                    for i, action in enumerate(self.actions):\n",
    "                        r, coords, pa = self.grid.peek(state, action)\n",
    "                        v = vs[row, col]\n",
    "\n",
    "                        action_v[i] = (r + self.gamma * vs[coords][0])\n",
    "                    maxInd = rand_argmax(np.array(action_v))\n",
    "                    best[row][col]['v'] = action_v[maxInd]\n",
    "                    best[row][col]['action'] = self.actions[maxInd]\n",
    "                    vs[row][col] = action_v[maxInd]\n",
    "                    dif = abs(v - vs[row][col])\n",
    "                    delta = max(delta, dif)\n",
    "        return best, vs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridWorld(5, 5, 1, 1, -1)\n",
    "policy = RandomPolicy(['up', 'down', 'left', 'right'])\n",
    "grid.set_special(row=0,\n",
    "                 column=1,\n",
    "                 reward=10,\n",
    "                 actions=policy.actions,\n",
    "                 targetcol=1,\n",
    "                 targetrow=4)\n",
    "grid.set_special(row=0,\n",
    "                 column=3,\n",
    "                 reward=5,\n",
    "                 actions=policy.actions,\n",
    "                 targetcol=3,\n",
    "                 targetrow=2)\n",
    "dp = Solver(grid, policy, .9)\n",
    "f = dp.policyEvaluation(.0001)\n",
    "pol, vs = dp.policyImprovement(f, .00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing policy evaluation and policy improvement algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['right', 'up', 'left', 'up', 'left'],\n",
       "       ['right', 'up', 'up', 'left', 'left'],\n",
       "       ['right', 'up', 'up', 'up', 'up'],\n",
       "       ['right', 'up', 'up', 'up', 'up'],\n",
       "       ['right', 'up', 'up', 'up', 'up']], dtype='<U5')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([x.get('action') for x in pol.flatten()]).reshape(5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.31,  8.79,  4.43,  5.32,  1.49],\n",
       "       [ 1.52,  2.99,  2.25,  1.91,  0.55],\n",
       "       [ 0.05,  0.74,  0.67,  0.36, -0.4 ],\n",
       "       [-0.97, -0.44, -0.35, -0.59, -1.18],\n",
       "       [-1.86, -1.34, -1.23, -1.42, -1.97]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([round(x, 2) for x in f.flatten()]).reshape(5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.98, 24.42, 21.98, 19.42, 17.48],\n",
       "       [19.78, 21.98, 19.78, 17.8 , 16.02],\n",
       "       [17.8 , 19.78, 17.8 , 16.02, 14.42],\n",
       "       [16.02, 17.8 , 16.02, 14.42, 12.98],\n",
       "       [14.42, 16.02, 14.42, 12.98, 11.68]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([round(x, 4) for x in vs.flatten()]).reshape(5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing simply doing value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol, vs = dp.ValueIteration(.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['right', 'right', 'left', 'right', 'left'],\n",
       "       ['up', 'up', 'left', 'left', 'left'],\n",
       "       ['right', 'up', 'left', 'left', 'left'],\n",
       "       ['right', 'up', 'left', 'up', 'left'],\n",
       "       ['right', 'up', 'left', 'up', 'left']], dtype='<U5')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([x.get('action') for x in pol.flatten()]).reshape(5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.31,  8.79,  4.43,  5.32,  1.49],\n",
       "       [ 1.52,  2.99,  2.25,  1.91,  0.55],\n",
       "       [ 0.05,  0.74,  0.67,  0.36, -0.4 ],\n",
       "       [-0.97, -0.44, -0.35, -0.59, -1.18],\n",
       "       [-1.86, -1.34, -1.23, -1.42, -1.97]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([round(x, 4) for x in f.flatten()]).reshape(5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.98, 24.42, 21.98, 19.42, 17.48],\n",
       "       [19.78, 21.98, 19.78, 17.8 , 16.02],\n",
       "       [17.8 , 19.78, 17.8 , 16.02, 14.42],\n",
       "       [16.02, 17.8 , 16.02, 14.42, 12.98],\n",
       "       [14.42, 16.02, 14.42, 12.98, 11.68]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([round(x, 4) for x in vs.flatten()]).reshape(5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like they converge to the same value, but we achieve to do so with much less total operations with value iteration"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e99705b024d1afa3704c5c6e6bad0aa71f5161e693dd13240058dc51c3371d8c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('skillquery': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
